"""
LLMå¤„ç†å™¨æµ‹è¯•

æµ‹è¯•çº§åˆ«ï¼š
- å•å…ƒæµ‹è¯•ï¼šå•ä¸ªæ–¹æ³•æµ‹è¯•
- é›†æˆæµ‹è¯•ï¼šå®Œæ•´æµç¨‹æµ‹è¯•
- è¾¹ç•Œæµ‹è¯•ï¼šå¼‚å¸¸æƒ…å†µå’Œè¾¹ç•Œæ¡ä»¶
- æ€§èƒ½æµ‹è¯•ï¼šå¤§é‡æ•°æ®å¤„ç†
"""

import pytest
import asyncio
from unittest.mock import Mock, AsyncMock, patch, MagicMock
from typing import Dict, Any

from iris_memory.processing.llm_processor import (
    LLMMessageProcessor,
    LLMClassificationResult,
    LLMSummaryResult
)


# =============================================================================
# Fixtures
# =============================================================================

@pytest.fixture
def mock_astrbot_context():
    """æ¨¡æ‹ŸAstrBotä¸Šä¸‹æ–‡"""
    context = Mock()
    context.send_message = AsyncMock()
    return context


@pytest.fixture
def mock_llm_api():
    """æ¨¡æ‹ŸLLM API"""
    api = Mock()
    api.text_chat = AsyncMock(return_value={
        "text": '{"layer": "immediate", "confidence": 0.9, "reason": "test"}'
    })
    api.chat_completion = AsyncMock(return_value={
        "choices": [{"message": {"content": '{"summary": "test summary"}'}}]
    })
    return api


@pytest.fixture
def processor(mock_astrbot_context):
    """åŸºç¡€LLMå¤„ç†å™¨å®ä¾‹"""
    return LLMMessageProcessor(
        astrbot_context=mock_astrbot_context,
        max_tokens=200
    )


@pytest.fixture
def initialized_processor(mock_astrbot_context, mock_llm_api):
    """å·²åˆå§‹åŒ–çš„LLMå¤„ç†å™¨"""
    processor = LLMMessageProcessor(
        astrbot_context=mock_astrbot_context,
        max_tokens=200
    )
    processor.llm_api = mock_llm_api
    return processor


# =============================================================================
# åˆå§‹åŒ–æµ‹è¯•
# =============================================================================

class TestInitialization:
    """åˆå§‹åŒ–æµ‹è¯•"""
    
    @pytest.mark.asyncio
    async def test_initialize_success(self, mock_astrbot_context, mock_llm_api):
        """æµ‹è¯•æˆåŠŸåˆå§‹åŒ–"""
        # Create a mock module for astrbot.api
        import sys
        from types import ModuleType
        
        # Create mock modules
        astrbot_module = ModuleType('astrbot')
        astrbot_api_module = ModuleType('astrbot.api')
        astrbot_api_module.AstrBotApi = Mock(return_value=mock_llm_api)
        astrbot_module.api = astrbot_api_module
        
        # Add to sys.modules
        sys.modules['astrbot'] = astrbot_module
        sys.modules['astrbot.api'] = astrbot_api_module
        
        try:
            processor = LLMMessageProcessor(mock_astrbot_context)
            result = await processor.initialize()
            
            assert result is True
            # llm_api is now lazy-loaded on first use, not during initialize()
            # is_available() checks llm_api which is not set until first use
            assert processor.astrbot_context is not None
        finally:
            # Clean up
            sys.modules.pop('astrbot', None)
            sys.modules.pop('astrbot.api', None)
    
    @pytest.mark.asyncio
    async def test_initialize_no_context(self):
        """æµ‹è¯•æ— ä¸Šä¸‹æ–‡åˆå§‹åŒ–å¤±è´¥"""
        processor = LLMMessageProcessor(astrbot_context=None)
        result = await processor.initialize()
        
        assert result is False
        assert processor.llm_api is None
        assert processor.is_available() is False
    
    @pytest.mark.asyncio
    async def test_initialize_import_error(self, mock_astrbot_context):
        """æµ‹è¯•å¯¼å…¥é”™è¯¯å¤„ç†"""
        # initialize() ç°åœ¨ä½¿ç”¨å»¶è¿ŸåŠ è½½ç­–ç•¥ï¼Œä¸å†åœ¨åˆå§‹åŒ–æ—¶å¯¼å…¥
        # åªè¦æœ‰contextå°±ä¼šinitializeæˆåŠŸ
        processor = LLMMessageProcessor(mock_astrbot_context)
        result = await processor.initialize()
        
        assert result is True


# =============================================================================
# æ¶ˆæ¯åˆ†ç±»æµ‹è¯•
# =============================================================================

class TestMessageClassification:
    """æ¶ˆæ¯åˆ†ç±»æµ‹è¯•"""
    
    @pytest.mark.asyncio
    async def test_classify_message_immediate(self, initialized_processor):
        """æµ‹è¯•é«˜ä¼˜å…ˆçº§æ¶ˆæ¯åˆ†ç±»"""
        initialized_processor.llm_api.text_chat.return_value = {
            "text": '{"layer": "immediate", "confidence": 0.9, "reason": "é‡è¦ä¿¡æ¯"}'
        }
        
        result = await initialized_processor.classify_message("æˆ‘å–œæ¬¢çŒ«")
        
        assert result is not None
        assert result.layer == "immediate"
        assert result.confidence == 0.9
        assert result.reason == "é‡è¦ä¿¡æ¯"
    
    @pytest.mark.asyncio
    async def test_classify_message_batch(self, initialized_processor):
        """æµ‹è¯•æ™®é€šæ¶ˆæ¯åˆ†ç±»"""
        initialized_processor.llm_api.text_chat.return_value = {
            "text": '{"layer": "batch", "confidence": 0.5, "reason": "æ™®é€šå¯¹è¯"}'
        }
        
        result = await initialized_processor.classify_message("ä»Šå¤©å¤©æ°”ä¸é”™")
        
        assert result is not None
        assert result.layer == "batch"
        assert result.confidence == 0.5
    
    @pytest.mark.asyncio
    async def test_classify_message_discard(self, initialized_processor):
        """æµ‹è¯•ä¸¢å¼ƒæ¶ˆæ¯åˆ†ç±»"""
        initialized_processor.llm_api.text_chat.return_value = {
            "text": '{"layer": "discard", "confidence": 0.1, "reason": "æ— æ„ä¹‰"}'
        }
        
        result = await initialized_processor.classify_message("å“ˆå“ˆ")
        
        assert result is not None
        assert result.layer == "discard"
    
    @pytest.mark.asyncio
    async def test_classify_message_invalid_json(self, initialized_processor):
        """æµ‹è¯•æ— æ•ˆJSONå“åº”å¤„ç†"""
        initialized_processor.llm_api.text_chat.return_value = {
            "text": "invalid json response"
        }
        
        result = await initialized_processor.classify_message("æµ‹è¯•æ¶ˆæ¯")
        
        # åº”è¯¥è¿”å›Noneæˆ–å›é€€å¤„ç†
        assert result is None
    
    @pytest.mark.asyncio
    async def test_classify_message_api_error(self, initialized_processor):
        """æµ‹è¯•APIé”™è¯¯å¤„ç†"""
        initialized_processor.llm_api.text_chat.side_effect = Exception("API Error")
        
        result = await initialized_processor.classify_message("æµ‹è¯•æ¶ˆæ¯")
        
        assert result is None
    
    @pytest.mark.asyncio
    async def test_classify_message_no_api(self, processor):
        """æµ‹è¯•æ— APIæƒ…å†µ"""
        result = await processor.classify_message("æµ‹è¯•æ¶ˆæ¯")
        
        assert result is None
    
    @pytest.mark.asyncio
    async def test_classify_message_with_context(self, initialized_processor):
        """æµ‹è¯•å¸¦ä¸Šä¸‹æ–‡çš„åˆ†ç±»"""
        initialized_processor.llm_api.text_chat.return_value = {
            "text": '{"layer": "immediate", "confidence": 0.85, "reason": "æœ‰ä¸Šä¸‹æ–‡"}'
        }
        
        context = {
            "session_message_count": 5,
            "last_topic": "å® ç‰©"
        }
        
        result = await initialized_processor.classify_message(
            "æˆ‘å–œæ¬¢çŒ«", context=context
        )
        
        assert result is not None
        # éªŒè¯ä¸Šä¸‹æ–‡è¢«åŒ…å«åœ¨promptä¸­
        call_args = initialized_processor.llm_api.text_chat.call_args
        assert call_args is not None


# =============================================================================
# æ‘˜è¦ç”Ÿæˆæµ‹è¯•
# =============================================================================

class TestSummaryGeneration:
    """æ‘˜è¦ç”Ÿæˆæµ‹è¯•"""
    
    @pytest.mark.asyncio
    async def test_generate_summary_success(self, initialized_processor):
        """æµ‹è¯•æˆåŠŸç”Ÿæˆæ‘˜è¦"""
        initialized_processor.llm_api.text_chat.return_value = {
            "text": '{"summary": "ç”¨æˆ·å–œæ¬¢çŒ«å’Œç‹—", "key_points": ["å–œæ¬¢çŒ«", "å–œæ¬¢ç‹—"], "user_preferences": ["å® ç‰©çˆ±å¥½è€…"]}'
        }
        
        messages = ["æˆ‘å–œæ¬¢çŒ«", "æˆ‘ä¹Ÿå–œæ¬¢ç‹—", "å®ƒä»¬å¾ˆå¯çˆ±"]
        result = await initialized_processor.generate_summary(
            messages, user_id="test_user"
        )
        
        assert result is not None
        assert result.summary == "ç”¨æˆ·å–œæ¬¢çŒ«å’Œç‹—"
        assert len(result.key_points) == 2
        assert len(result.user_preferences) == 1
    
    @pytest.mark.asyncio
    async def test_generate_summary_empty_messages(self, initialized_processor):
        """æµ‹è¯•ç©ºæ¶ˆæ¯åˆ—è¡¨"""
        result = await initialized_processor.generate_summary(
            [], user_id="test_user"
        )
        
        assert result is None
    
    @pytest.mark.asyncio
    async def test_generate_summary_single_message(self, initialized_processor):
        """æµ‹è¯•å•æ¡æ¶ˆæ¯æ‘˜è¦"""
        initialized_processor.llm_api.text_chat.return_value = {
            "text": '{"summary": "ç”¨æˆ·å–œæ¬¢çŒ«", "key_points": ["å–œæ¬¢çŒ«"], "user_preferences": []}'
        }
        
        messages = ["æˆ‘å–œæ¬¢çŒ«"]
        result = await initialized_processor.generate_summary(
            messages, user_id="test_user"
        )
        
        assert result is not None
        assert "å–œæ¬¢çŒ«" in result.summary
    
    @pytest.mark.asyncio
    async def test_generate_summary_many_messages(self, initialized_processor):
        """æµ‹è¯•å¤§é‡æ¶ˆæ¯æ‘˜è¦"""
        initialized_processor.llm_api.text_chat.return_value = {
            "text": '{"summary": "å¯¹è¯æ‘˜è¦", "key_points": ["è¦ç‚¹1"], "user_preferences": []}'
        }
        
        # æµ‹è¯•è¶…è¿‡10æ¡æ¶ˆæ¯æ—¶åªå–æœ€è¿‘10æ¡
        # ç”Ÿæˆ15æ¡æ¶ˆæ¯ (ç´¢å¼• 0-14)
        messages = [f"æ¶ˆæ¯{i}" for i in range(15)]
        result = await initialized_processor.generate_summary(
            messages, user_id="test_user"
        )
        
        assert result is not None
        # éªŒè¯åªä½¿ç”¨äº†æœ€è¿‘10æ¡ (ç´¢å¼• 5-14)
        call_args = initialized_processor.llm_api.text_chat.call_args[1]
        prompt = call_args.get('prompt', '')
        assert "æ¶ˆæ¯4" not in prompt  # æ—§æ¶ˆæ¯ä¸åº”è¯¥åœ¨promptä¸­
        assert "æ¶ˆæ¯5" in prompt  # ç¬¬ä¸€æ¡ä¿ç•™çš„æ¶ˆæ¯
        assert "æ¶ˆæ¯14" in prompt  # æœ€åä¸€æ¡æ¶ˆæ¯
    
    @pytest.mark.asyncio
    async def test_generate_summary_with_persona(self, initialized_processor):
        """æµ‹è¯•å¸¦ç”¨æˆ·ç”»åƒçš„æ‘˜è¦"""
        initialized_processor.llm_api.text_chat.return_value = {
            "text": '{"summary": "ç¬¦åˆç”»åƒçš„æ‘˜è¦", "key_points": [], "user_preferences": []}'
        }
        
        context = {
            "user_persona": {"interests": ["å® ç‰©", "æ‘„å½±"]}
        }
        
        messages = ["æˆ‘å–œæ¬¢çŒ«"]
        result = await initialized_processor.generate_summary(
            messages, user_id="test_user", context=context
        )
        
        assert result is not None
        # éªŒè¯ç”¨æˆ·ç”»åƒè¢«åŒ…å«
        call_args = initialized_processor.llm_api.text_chat.call_args
        assert call_args is not None
    
    @pytest.mark.asyncio
    async def test_generate_summary_parse_fallback(self, initialized_processor):
        """æµ‹è¯•JSONè§£æå¤±è´¥æ—¶çš„å›é€€"""
        initialized_processor.llm_api.text_chat.return_value = {
            "text": "è¿™æ˜¯ä¸€ä¸ªçº¯æ–‡æœ¬å›å¤ï¼Œä¸æ˜¯JSONæ ¼å¼"
        }
        
        messages = ["æˆ‘å–œæ¬¢çŒ«"]
        result = await initialized_processor.generate_summary(
            messages, user_id="test_user"
        )
        
        # åº”è¯¥è¿”å›ä½¿ç”¨åŸå§‹æ–‡æœ¬çš„ç»“æœ
        assert result is not None
        assert result.summary == "è¿™æ˜¯ä¸€ä¸ªçº¯æ–‡æœ¬å›å¤ï¼Œä¸æ˜¯JSONæ ¼å¼"


# =============================================================================
# è¾¹ç•Œæ¡ä»¶æµ‹è¯•
# =============================================================================

class TestEdgeCases:
    """è¾¹ç•Œæ¡ä»¶æµ‹è¯•"""
    
    @pytest.mark.asyncio
    async def test_very_long_message_classification(self, initialized_processor):
        """æµ‹è¯•è¶…é•¿æ¶ˆæ¯åˆ†ç±»"""
        long_message = "æˆ‘å–œæ¬¢çŒ«" * 1000
        
        initialized_processor.llm_api.text_chat.return_value = {
            "text": '{"layer": "batch", "confidence": 0.5, "reason": "é•¿æ¶ˆæ¯"}'
        }
        
        result = await initialized_processor.classify_message(long_message)
        
        assert result is not None
    
    @pytest.mark.asyncio
    async def test_special_characters_in_message(self, initialized_processor):
        """æµ‹è¯•ç‰¹æ®Šå­—ç¬¦å¤„ç†"""
        special_message = "æˆ‘å–œæ¬¢çŒ«ï¼ğŸ± <script>alert('xss')</script> \\n\\t"
        
        initialized_processor.llm_api.text_chat.return_value = {
            "text": '{"layer": "immediate", "confidence": 0.8, "reason": "ç‰¹æ®Šå­—ç¬¦"}'
        }
        
        result = await initialized_processor.classify_message(special_message)
        
        assert result is not None
    
    @pytest.mark.asyncio
    async def test_unicode_message(self, initialized_processor):
        """æµ‹è¯•Unicodeæ¶ˆæ¯"""
        unicode_message = "æˆ‘å–œæ¬¢çŒ«ğŸ± dogs sÃ£o legais æ—¥æœ¬èª"
        
        initialized_processor.llm_api.text_chat.return_value = {
            "text": '{"layer": "immediate", "confidence": 0.9, "reason": "unicode"}'
        }
        
        result = await initialized_processor.classify_message(unicode_message)
        
        assert result is not None
    
    @pytest.mark.asyncio
    async def test_concurrent_requests(self, initialized_processor):
        """æµ‹è¯•å¹¶å‘è¯·æ±‚å¤„ç†"""
        initialized_processor.llm_api.text_chat.return_value = {
            "text": '{"layer": "batch", "confidence": 0.5, "reason": "å¹¶å‘"}'
        }
        
        # å¹¶å‘å‘é€å¤šä¸ªè¯·æ±‚
        tasks = [
            initialized_processor.classify_message(f"æ¶ˆæ¯{i}")
            for i in range(10)
        ]
        
        results = await asyncio.gather(*tasks)
        
        # æ‰€æœ‰è¯·æ±‚éƒ½åº”è¯¥æˆåŠŸ
        assert all(r is not None for r in results)
        assert len(results) == 10


# =============================================================================
# ç»Ÿè®¡ä¿¡æ¯æµ‹è¯•
# =============================================================================

class TestStatistics:
    """ç»Ÿè®¡ä¿¡æ¯æµ‹è¯•"""
    
    @pytest.mark.asyncio
    async def test_stats_tracking(self, initialized_processor):
        """æµ‹è¯•ç»Ÿè®¡ä¿¡æ¯è¿½è¸ª"""
        # åˆå§‹çŠ¶æ€
        stats = initialized_processor.get_stats()
        assert stats["classification_calls"] == 0
        assert stats["summary_calls"] == 0
        
        # æ‰§è¡Œä¸€äº›æ“ä½œ
        initialized_processor.llm_api.text_chat.return_value = {
            "text": '{"layer": "immediate", "confidence": 0.9, "reason": "test"}'
        }
        
        await initialized_processor.classify_message("æµ‹è¯•")
        
        stats = initialized_processor.get_stats()
        assert stats["classification_calls"] == 1
        assert stats["failed_calls"] == 0
    
    @pytest.mark.asyncio
    async def test_stats_after_failure(self, initialized_processor):
        """æµ‹è¯•å¤±è´¥åçš„ç»Ÿè®¡"""
        initialized_processor.llm_api.text_chat.side_effect = Exception("Error")
        
        await initialized_processor.classify_message("æµ‹è¯•")
        
        stats = initialized_processor.get_stats()
        assert stats["failed_calls"] == 1


# =============================================================================
# JSONè§£ææµ‹è¯•
# =============================================================================

class TestJSONParsing:
    """JSONè§£ææµ‹è¯•"""
    
    def test_parse_valid_json(self, initialized_processor):
        """æµ‹è¯•æœ‰æ•ˆJSONè§£æ"""
        response = '{"layer": "immediate", "confidence": 0.9}'
        result = initialized_processor._parse_json_response(response)
        
        assert result is not None
        assert result["layer"] == "immediate"
    
    def test_parse_json_with_code_block(self, initialized_processor):
        """æµ‹è¯•ä»£ç å—ä¸­çš„JSON"""
        response = '```json\n{"layer": "batch"}\n```'
        result = initialized_processor._parse_json_response(response)
        
        assert result is not None
        assert result["layer"] == "batch"
    
    def test_parse_json_with_extra_text(self, initialized_processor):
        """æµ‹è¯•å¸¦é¢å¤–æ–‡æœ¬çš„JSON"""
        response = 'Here is the result: {"layer": "discard"} Thanks!'
        result = initialized_processor._parse_json_response(response)
        
        assert result is not None
        assert result["layer"] == "discard"
    
    def test_parse_invalid_json(self, initialized_processor):
        """æµ‹è¯•æ— æ•ˆJSON"""
        response = "This is not JSON"
        result = initialized_processor._parse_json_response(response)
        
        assert result is None
    
    def test_parse_empty_response(self, initialized_processor):
        """æµ‹è¯•ç©ºå“åº”"""
        result = initialized_processor._parse_json_response("")
        
        assert result is None


# =============================================================================
# é…ç½®æµ‹è¯•
# =============================================================================

class TestConfiguration:
    """é…ç½®æµ‹è¯•"""
    
    def test_custom_prompts(self, mock_astrbot_context):
        """æµ‹è¯•è‡ªå®šä¹‰æç¤ºè¯"""
        custom_class_prompt = "Custom classification prompt"
        custom_summary_prompt = "Custom summary prompt"
        
        processor = LLMMessageProcessor(
            astrbot_context=mock_astrbot_context,
            classification_prompt=custom_class_prompt,
            summary_prompt=custom_summary_prompt,
            max_tokens=300
        )
        
        assert processor.classification_prompt == custom_class_prompt
        assert processor.summary_prompt == custom_summary_prompt
        assert processor.max_tokens == 300
    
    def test_default_prompts(self, mock_astrbot_context):
        """æµ‹è¯•é»˜è®¤æç¤ºè¯"""
        processor = LLMMessageProcessor(astrbot_context=mock_astrbot_context)
        
        assert "layer" in processor.classification_prompt
        assert "summary" in processor.summary_prompt


# =============================================================================
# æ€§èƒ½æµ‹è¯•
# =============================================================================

@pytest.mark.slow
class TestPerformance:
    """æ€§èƒ½æµ‹è¯•"""
    
    @pytest.mark.asyncio
    async def test_large_batch_processing(self, initialized_processor):
        """æµ‹è¯•å¤§æ‰¹é‡å¤„ç†æ€§èƒ½"""
        initialized_processor.llm_api.text_chat.return_value = {
            "text": '{"layer": "batch", "confidence": 0.5}'
        }
        
        # å¤„ç†100æ¡æ¶ˆæ¯
        start_time = asyncio.get_event_loop().time()
        
        for i in range(100):
            await initialized_processor.classify_message(f"æ¶ˆæ¯{i}")
        
        elapsed = asyncio.get_event_loop().time() - start_time
        
        # åº”è¯¥åœ¨åˆç†æ—¶é—´å†…å®Œæˆï¼ˆå‡è®¾æ¯ç§’10ä¸ªè¯·æ±‚ï¼‰
        assert elapsed < 15  # æ”¾å®½åˆ°15ç§’
    
    @pytest.mark.asyncio
    async def test_memory_usage_with_large_messages(self, initialized_processor):
        """æµ‹è¯•å¤§æ¶ˆæ¯çš„å†…å­˜ä½¿ç”¨"""
        large_message = "A" * 10000  # 10KBæ¶ˆæ¯
        
        initialized_processor.llm_api.text_chat.return_value = {
            "text": '{"layer": "batch"}'
        }
        
        result = await initialized_processor.classify_message(large_message)
        
        assert result is not None


if __name__ == "__main__":
    pytest.main([__file__, "-v"])
